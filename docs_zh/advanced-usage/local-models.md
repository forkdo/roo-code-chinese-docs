---
description: 了解如何使用 Ollama 和 LM Studio 在本地 AI 模型上运行 Roo Code。完整的离线 AI 编程辅助设置指南。
keywords:
  - 本地模型
  - Ollama
  - LM Studio
  - 离线 AI
  - 本地大语言模型
  - 自托管 AI
  - 隐私优先 AI
image: /img/social-share.jpg
---

# 使用本地模型

Roo Code 支持使用 [Ollama](https://ollama.com/) 和 [LM Studio](https://lmstudio.ai/) 在您自己的计算机上本地运行语言模型。这提供了几个优势：

*   **隐私性：** 您的代码和数据永远不会离开您的计算机。
*   **离线访问：** 即使没有互联网连接，您也可以使用 Roo Code。
*   **节省成本：** 避免与基于云的模型相关的 API 使用费用。
*   **可定制性：** 可以尝试不同的模型和配置。

**但是，使用本地模型也存在一些缺点：**

*   **资源需求：** 本地模型可能消耗大量资源，需要功能强大的计算机，配备良好的 CPU，理想情况下还需要独立 GPU。
*   **设置复杂性：** 与使用基于云的 API 相比，设置本地模型可能更复杂。
*   **模型性能：** 本地模型的性能可能差异很大。虽然某些模型表现出色，但它们可能并不总是能与最大、最先进的云模型相媲美。
* **功能有限**：本地模型（以及许多在线模型）通常不支持高级功能，例如提示缓存、计算机使用等。

---

## 支持的本地模型提供商

Roo Code 目前支持两个主要的本地模型提供商：

1.  **Ollama：** 一个流行的开源工具，用于在本地运行大语言模型。它支持多种模型。
2.  **LM Studio：** 一个用户友好的桌面应用程序，可简化下载、配置和运行本地模型的过程。它还提供了一个模拟 OpenAI API 的本地服务器。

---

## 设置本地模型

有关详细的设置说明，请参阅：
* [设置 Ollama](/providers/ollama)
* [设置 LM Studio](/providers/lmstudio)

这两个提供商提供类似的功能，但具有不同的用户界面和工作流程。Ollama 通过其命令行界面提供更多的控制，而 LM Studio 则提供更用户友好的图形界面。

---

## 故障排除

*   **"由于目标机器主动拒绝，无法建立连接"：** 这通常意味着 Ollama 或 LM Studio 服务器未运行，或者正在运行的端口/地址与 Roo Code 配置的不一致。请仔细检查 Base URL 设置。

*   **响应速度慢：** 本地模型可能比基于云的模型慢，尤其是在性能较低的硬件上。如果性能是问题，请尝试使用较小的模型。

*   **找不到模型：** 确保您输入的模型名称正确。如果您使用 Ollama，请使用与 `ollama run` 命令中提供的相同名称。